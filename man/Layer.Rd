% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/simpleNN.R
\name{Layer}
\alias{Layer}
\title{Fully connected layer for neural network}
\usage{
Layer(activation, minibatchSize, sizeP, is_input = FALSE, is_output = FALSE,
  initPos, initScale)
}
\arguments{
\item{activation}{function for neural network. Must be able to do elementwise
calculations on a matrix and have the parameter \code{deriv},
which is a boolean indicating whether the derivative should be
calculated}

\item{minibatchSize}{Number of samples used for estimating the gradient}

\item{sizeP}{vector of two values, number of inputs to this layer and number
of outputs from this layer, ignoring bias values.}

\item{is_input}{boolean indicating whether this is an input}

\item{is_output}{boolean indicating whether this is output}

\item{initPos}{boolean indicating whether weights should be initialized as positive}

\item{initScale}{scalar for initialising wieghts, e.g. if it is 100, then
the randomly sampled initial weights are scaled by 1/100.}
}
\value{
environment with the functions to set all the internal matricies and
        a function to forward propagate through the layer.
}
\description{
\code{Layer} encapsulates all the data needed for a fully connected layer.
}
\examples{
testLayer <- Layer(mnistr::reLU, 3, c(10,10),FALSE,FALSE,TRUE,1000)
testLayer$W$getter() # Check random weights
}
\keyword{internal}
