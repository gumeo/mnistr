% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/simpleNN.R
\name{mlp}
\alias{mlp}
\title{Multi Layer Percepteron}
\usage{
mlp(structNet, minibatchSize, activation, initPos = FALSE, initScale = 100)
}
\arguments{
\item{structNet}{Vector inidcating the sizes of the nodes in the network. E.g.
\code{c(100,70,40,10)} would be a network with 100 input nodes,
2 hidden layers with 70 and 40 neurons respectively, and an 
output layer with 10 neurons.}

\item{minibatchSize}{Number of samples used for estimating the gradient}

\item{activation}{function for neural network. Must be able to do elementwise
calculations on a matrix and have the parameter \code{deriv},
which is a boolean indicating whether the derivative should be
calculated}

\item{initPos}{boolean indicating whether weights should be initialized as positive}

\item{initScale}{scalar for initialising wieghts, e.g. if it is 100, then
the randomly sampled initial weights are scaled by 1/100.}
}
\value{
environment with the functions to train the network, forwards propagate and
        a list with all the layers. The forward propage function can be used to do
        predictions.
}
\description{
\code{mlp} is a function that generates an MLP, for which you can train on data.
}
\examples{
testMLP <- mlp(c(10,10,10),5,mnistr::reLU,TRUE,1000)
testLayer$W$getter() # Check random weights
}
